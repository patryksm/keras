{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "np.random.seed(1)\n",
    "env.seed(1)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0922 23:21:04.956418  4268 deprecation_wrapper.py:119] From C:\\Users\\Patryk\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0922 23:21:04.979385  4268 deprecation_wrapper.py:119] From C:\\Users\\Patryk\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0922 23:21:04.993319  4268 deprecation_wrapper.py:119] From C:\\Users\\Patryk\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 658\n",
      "Trainable params: 658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model = model,\n",
    "               nb_actions = nb_actions,\n",
    "               memory = memory,\n",
    "               nb_steps_warmup = 10,\n",
    "               target_model_update = 1e-2,\n",
    "               policy = policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0922 23:21:08.169809  4268 deprecation_wrapper.py:119] From C:\\Users\\Patryk\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0922 23:21:08.170784  4268 deprecation_wrapper.py:119] From C:\\Users\\Patryk\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0922 23:21:10.396534  4268 deprecation_wrapper.py:119] From C:\\Users\\Patryk\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patryk\\Anaconda3\\lib\\site-packages\\rl\\memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  19/1000: episode: 1, duration: 1.713s, episode steps: 19, steps per second: 11, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.066 [-1.176, 1.899], loss: 0.489367, mean_absolute_error: 0.539168, mean_q: 0.140377\n",
      "  36/1000: episode: 2, duration: 0.144s, episode steps: 17, steps per second: 118, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.107 [-0.984, 0.363], loss: 0.369588, mean_absolute_error: 0.541507, mean_q: 0.366107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patryk\\Anaconda3\\lib\\site-packages\\rl\\memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  45/1000: episode: 3, duration: 0.076s, episode steps: 9, steps per second: 118, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.148 [-1.182, 1.984], loss: 0.282838, mean_absolute_error: 0.531025, mean_q: 0.575017\n",
      "  56/1000: episode: 4, duration: 0.088s, episode steps: 11, steps per second: 125, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.096 [-1.036, 1.776], loss: 0.206385, mean_absolute_error: 0.592027, mean_q: 0.883662\n",
      "  65/1000: episode: 5, duration: 0.075s, episode steps: 9, steps per second: 121, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.142 [-1.174, 1.887], loss: 0.163944, mean_absolute_error: 0.654675, mean_q: 1.171511\n",
      "  74/1000: episode: 6, duration: 0.076s, episode steps: 9, steps per second: 119, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.135 [-1.419, 2.310], loss: 0.143369, mean_absolute_error: 0.711976, mean_q: 1.348378\n",
      " 105/1000: episode: 7, duration: 0.258s, episode steps: 31, steps per second: 120, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.387 [0.000, 1.000], mean observation: 0.040 [-1.363, 2.330], loss: 0.149531, mean_absolute_error: 0.739554, mean_q: 1.325665\n",
      " 114/1000: episode: 8, duration: 0.074s, episode steps: 9, steps per second: 121, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.144 [-0.969, 1.763], loss: 0.121563, mean_absolute_error: 0.825037, mean_q: 1.593463\n",
      " 134/1000: episode: 9, duration: 0.167s, episode steps: 20, steps per second: 120, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.092 [-0.824, 1.779], loss: 0.116862, mean_absolute_error: 0.879764, mean_q: 1.669141\n",
      " 149/1000: episode: 10, duration: 0.125s, episode steps: 15, steps per second: 120, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.089 [-1.012, 1.578], loss: 0.108305, mean_absolute_error: 0.954371, mean_q: 1.890506\n",
      " 167/1000: episode: 11, duration: 0.149s, episode steps: 18, steps per second: 121, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.053 [-0.992, 1.561], loss: 0.100991, mean_absolute_error: 1.024333, mean_q: 1.993965\n",
      " 177/1000: episode: 12, duration: 0.083s, episode steps: 10, steps per second: 120, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.141 [-1.944, 1.134], loss: 0.082387, mean_absolute_error: 1.067153, mean_q: 2.153957\n",
      " 198/1000: episode: 13, duration: 0.175s, episode steps: 21, steps per second: 120, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.090 [-0.769, 1.451], loss: 0.093921, mean_absolute_error: 1.148643, mean_q: 2.276837\n",
      " 216/1000: episode: 14, duration: 0.150s, episode steps: 18, steps per second: 120, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.066 [-1.545, 2.419], loss: 0.104374, mean_absolute_error: 1.214637, mean_q: 2.345386\n",
      " 233/1000: episode: 15, duration: 0.142s, episode steps: 17, steps per second: 120, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.085 [-1.220, 2.064], loss: 0.121655, mean_absolute_error: 1.290105, mean_q: 2.482538\n",
      " 276/1000: episode: 16, duration: 0.359s, episode steps: 43, steps per second: 120, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.129 [-0.792, 1.296], loss: 0.112894, mean_absolute_error: 1.394662, mean_q: 2.688576\n",
      " 288/1000: episode: 17, duration: 0.099s, episode steps: 12, steps per second: 121, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.094 [-1.193, 1.950], loss: 0.112866, mean_absolute_error: 1.522337, mean_q: 2.969637\n",
      " 307/1000: episode: 18, duration: 0.159s, episode steps: 19, steps per second: 120, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.019 [-1.600, 2.238], loss: 0.122867, mean_absolute_error: 1.555422, mean_q: 2.958856\n",
      " 325/1000: episode: 19, duration: 0.155s, episode steps: 18, steps per second: 116, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.045 [-1.607, 2.302], loss: 0.113212, mean_absolute_error: 1.630595, mean_q: 3.119973\n",
      " 345/1000: episode: 20, duration: 0.161s, episode steps: 20, steps per second: 124, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.051 [-1.192, 1.698], loss: 0.151400, mean_absolute_error: 1.715168, mean_q: 3.237511\n",
      " 355/1000: episode: 21, duration: 0.083s, episode steps: 10, steps per second: 121, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.109 [-1.939, 1.223], loss: 0.138118, mean_absolute_error: 1.776564, mean_q: 3.364228\n",
      " 382/1000: episode: 22, duration: 0.227s, episode steps: 27, steps per second: 119, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.407 [0.000, 1.000], mean observation: 0.012 [-1.409, 1.920], loss: 0.137949, mean_absolute_error: 1.850933, mean_q: 3.510544\n",
      " 401/1000: episode: 23, duration: 0.157s, episode steps: 19, steps per second: 121, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.047 [-1.343, 2.060], loss: 0.125135, mean_absolute_error: 1.962090, mean_q: 3.768055\n",
      " 444/1000: episode: 24, duration: 0.357s, episode steps: 43, steps per second: 120, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.419 [0.000, 1.000], mean observation: 0.044 [-1.354, 2.265], loss: 0.134864, mean_absolute_error: 2.067356, mean_q: 3.965947\n",
      " 479/1000: episode: 25, duration: 0.292s, episode steps: 35, steps per second: 120, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.057 [-1.016, 1.965], loss: 0.148133, mean_absolute_error: 2.236872, mean_q: 4.287086\n",
      " 497/1000: episode: 26, duration: 0.150s, episode steps: 18, steps per second: 120, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.066 [-1.001, 1.573], loss: 0.161236, mean_absolute_error: 2.316870, mean_q: 4.443219\n",
      " 546/1000: episode: 27, duration: 0.409s, episode steps: 49, steps per second: 120, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.036 [-1.348, 1.842], loss: 0.130806, mean_absolute_error: 2.483541, mean_q: 4.777874\n",
      " 564/1000: episode: 28, duration: 0.150s, episode steps: 18, steps per second: 120, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.116 [-1.227, 0.619], loss: 0.139043, mean_absolute_error: 2.613991, mean_q: 5.042856\n",
      " 576/1000: episode: 29, duration: 0.099s, episode steps: 12, steps per second: 121, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.076 [-1.620, 1.029], loss: 0.188299, mean_absolute_error: 2.637300, mean_q: 5.059541\n",
      " 597/1000: episode: 30, duration: 0.175s, episode steps: 21, steps per second: 120, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.089 [-0.773, 1.159], loss: 0.174853, mean_absolute_error: 2.717588, mean_q: 5.250879\n",
      " 639/1000: episode: 31, duration: 0.350s, episode steps: 42, steps per second: 120, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.002 [-1.297, 0.987], loss: 0.276723, mean_absolute_error: 2.818382, mean_q: 5.355421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 665/1000: episode: 32, duration: 0.218s, episode steps: 26, steps per second: 119, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.016 [-1.849, 1.165], loss: 0.194620, mean_absolute_error: 2.950017, mean_q: 5.634647\n",
      " 700/1000: episode: 33, duration: 0.289s, episode steps: 35, steps per second: 121, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.087 [-0.588, 0.857], loss: 0.274268, mean_absolute_error: 3.082879, mean_q: 5.893820\n",
      " 713/1000: episode: 34, duration: 0.109s, episode steps: 13, steps per second: 120, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.116 [-1.384, 0.771], loss: 0.147221, mean_absolute_error: 3.154913, mean_q: 6.068291\n",
      " 726/1000: episode: 35, duration: 0.108s, episode steps: 13, steps per second: 120, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.067 [-1.196, 1.724], loss: 0.371808, mean_absolute_error: 3.189901, mean_q: 6.099807\n",
      " 740/1000: episode: 36, duration: 0.115s, episode steps: 14, steps per second: 121, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.108 [-1.161, 1.976], loss: 0.190029, mean_absolute_error: 3.243716, mean_q: 6.298890\n",
      " 755/1000: episode: 37, duration: 0.125s, episode steps: 15, steps per second: 120, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.070 [-0.982, 1.584], loss: 0.369092, mean_absolute_error: 3.277977, mean_q: 6.246489\n",
      " 791/1000: episode: 38, duration: 0.300s, episode steps: 36, steps per second: 120, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.079 [-0.504, 0.924], loss: 0.305507, mean_absolute_error: 3.407003, mean_q: 6.580600\n",
      " 809/1000: episode: 39, duration: 0.150s, episode steps: 18, steps per second: 120, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.101 [-0.609, 1.132], loss: 0.359675, mean_absolute_error: 3.502121, mean_q: 6.788415\n",
      " 833/1000: episode: 40, duration: 0.198s, episode steps: 24, steps per second: 121, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.039 [-1.254, 0.817], loss: 0.442150, mean_absolute_error: 3.557021, mean_q: 6.774217\n",
      " 859/1000: episode: 41, duration: 0.217s, episode steps: 26, steps per second: 120, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.122 [-1.521, 0.811], loss: 0.329472, mean_absolute_error: 3.675768, mean_q: 7.110139\n",
      " 871/1000: episode: 42, duration: 0.100s, episode steps: 12, steps per second: 120, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.111 [-1.011, 1.706], loss: 0.487346, mean_absolute_error: 3.687371, mean_q: 7.063867\n",
      " 892/1000: episode: 43, duration: 0.175s, episode steps: 21, steps per second: 120, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.092 [-0.424, 0.905], loss: 0.524317, mean_absolute_error: 3.809826, mean_q: 7.335426\n",
      " 908/1000: episode: 44, duration: 0.134s, episode steps: 16, steps per second: 120, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.069 [-1.457, 0.961], loss: 0.594501, mean_absolute_error: 3.830609, mean_q: 7.329416\n",
      " 930/1000: episode: 45, duration: 0.185s, episode steps: 22, steps per second: 119, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.100 [-0.913, 0.361], loss: 0.573570, mean_absolute_error: 3.934997, mean_q: 7.564393\n",
      " 950/1000: episode: 46, duration: 0.164s, episode steps: 20, steps per second: 122, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.055 [-0.986, 1.711], loss: 0.421931, mean_absolute_error: 3.992477, mean_q: 7.752809\n",
      " 962/1000: episode: 47, duration: 0.099s, episode steps: 12, steps per second: 121, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.094 [-0.806, 1.302], loss: 0.362563, mean_absolute_error: 4.087401, mean_q: 7.997526\n",
      " 999/1000: episode: 48, duration: 0.309s, episode steps: 37, steps per second: 120, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.076 [-1.146, 0.528], loss: 0.536341, mean_absolute_error: 4.147640, mean_q: 8.048807\n",
      "done, took 9.899 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2753bb7c8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=1000, visualize=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('dqn_{}_weights.h5f'.format('CartPole-v0'), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 58.000, steps: 58\n",
      "Episode 2: reward: 69.000, steps: 69\n",
      "Episode 3: reward: 59.000, steps: 59\n",
      "Episode 4: reward: 43.000, steps: 43\n",
      "Episode 5: reward: 59.000, steps: 59\n",
      "Episode 6: reward: 45.000, steps: 45\n",
      "Episode 7: reward: 60.000, steps: 60\n",
      "Episode 8: reward: 56.000, steps: 56\n",
      "Episode 9: reward: 64.000, steps: 64\n",
      "Episode 10: reward: 50.000, steps: 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a28bdc1188>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
