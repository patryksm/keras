{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W tym notebooku importujemy Keras oraz bibliotekę reinforcement learning i korzystamy z backendu Tensorflow. Do rozwiązania problemu przedstawionego w poprzednim notebooku użyjemy algorytmu Deep Q-learning, który dostosuje wagi na podstawie danych ze środowiska."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nb_actions będzie zawierać informacje o wszystkich dostępnych możliwych akcjach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "np.random.seed(1)\n",
    "env.seed(1)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutaj budujemy faktyczny model sekwencyjny i dodajemy do niego kolejne warstwy, np. flatten zmienia macierz w prosty array. Po czym klasyczne warstwy Dense, z 20 neuronami i rektyfikowaną jednostką liniową jako naszą funkcje aktywacyjną do każdej z warstw. Gdzie na końcu liczba neuronów będzie równa zmiennej nb_actions określonej wcześniej, przy liniowej funkcji aktywacyjnej (rektyfikowana nie ma tu sensu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0207 01:35:35.586518 13380 deprecation_wrapper.py:119] From C:\\Users\\Patryk\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0207 01:35:35.605467 13380 deprecation_wrapper.py:119] From C:\\Users\\Patryk\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0207 01:35:35.617435 13380 deprecation_wrapper.py:119] From C:\\Users\\Patryk\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                100       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 42        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 982\n",
      "Trainable params: 982\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(20))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(20))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(20))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać powyżej, nasza sieć neuronowa jest już gotowa do użycia. Teraz należy skonfigurować algorytm reinforcement learning. Zmienną memory tworzymy po to, żeby nasz agent DQN zapamiętywał poprzednie akcje w czasie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model = model,\n",
    "               nb_actions = nb_actions,\n",
    "               memory = memory,\n",
    "               nb_steps_warmup = 10,\n",
    "               target_model_update = 1e-2,\n",
    "               policy = policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie dokonujemy kompilacji przy użyciu algorytmu optymalizującego Adam, który jest obecnie najczęściej używany. Przy czym błąd określany jest przez mean absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0207 01:35:35.724150 13380 deprecation_wrapper.py:119] From C:\\Users\\Patryk\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0207 01:35:35.724150 13380 deprecation_wrapper.py:119] From C:\\Users\\Patryk\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0207 01:35:37.597707 13380 deprecation_wrapper.py:119] From C:\\Users\\Patryk\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutaj trenujemy naszą sieć w środowisku CartPole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patryk\\Anaconda3\\lib\\site-packages\\rl\\memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  19/1000: episode: 1, duration: 1.534s, episode steps: 19, steps per second: 12, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.066 [-1.176, 1.899], loss: 0.478723, mean_absolute_error: 0.578019, mean_q: 0.211937\n",
      "  36/1000: episode: 2, duration: 0.141s, episode steps: 17, steps per second: 121, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.107 [-0.984, 0.363], loss: 0.326331, mean_absolute_error: 0.575049, mean_q: 0.458034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patryk\\Anaconda3\\lib\\site-packages\\rl\\memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  45/1000: episode: 3, duration: 0.076s, episode steps: 9, steps per second: 118, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.148 [-1.182, 1.984], loss: 0.236224, mean_absolute_error: 0.593429, mean_q: 0.689771\n",
      "  59/1000: episode: 4, duration: 0.115s, episode steps: 14, steps per second: 121, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.083 [-0.845, 1.586], loss: 0.179544, mean_absolute_error: 0.686464, mean_q: 1.029197\n",
      "  79/1000: episode: 5, duration: 0.165s, episode steps: 20, steps per second: 121, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.094 [-1.185, 2.169], loss: 0.152595, mean_absolute_error: 0.715630, mean_q: 1.196697\n",
      "  97/1000: episode: 6, duration: 0.151s, episode steps: 18, steps per second: 119, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.069 [-1.450, 0.931], loss: 0.155953, mean_absolute_error: 0.794378, mean_q: 1.389115\n",
      " 109/1000: episode: 7, duration: 0.101s, episode steps: 12, steps per second: 119, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.121 [-1.942, 3.097], loss: 0.134030, mean_absolute_error: 0.804175, mean_q: 1.479003\n",
      " 122/1000: episode: 8, duration: 0.107s, episode steps: 13, steps per second: 121, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.127 [-0.971, 1.831], loss: 0.133033, mean_absolute_error: 0.856621, mean_q: 1.680414\n",
      " 137/1000: episode: 9, duration: 0.125s, episode steps: 15, steps per second: 120, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.074 [-1.391, 2.256], loss: 0.150339, mean_absolute_error: 0.937166, mean_q: 1.784732\n",
      " 146/1000: episode: 10, duration: 0.075s, episode steps: 9, steps per second: 120, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.129 [-1.207, 1.871], loss: 0.097257, mean_absolute_error: 1.004711, mean_q: 2.027304\n",
      " 164/1000: episode: 11, duration: 0.150s, episode steps: 18, steps per second: 120, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.063 [-1.194, 2.034], loss: 0.107161, mean_absolute_error: 1.076371, mean_q: 2.193929\n",
      " 174/1000: episode: 12, duration: 0.083s, episode steps: 10, steps per second: 120, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.147 [-2.606, 1.526], loss: 0.153224, mean_absolute_error: 1.141215, mean_q: 2.209728\n",
      " 185/1000: episode: 13, duration: 0.091s, episode steps: 11, steps per second: 120, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.125 [-1.355, 2.080], loss: 0.099331, mean_absolute_error: 1.184428, mean_q: 2.395560\n",
      " 211/1000: episode: 14, duration: 0.216s, episode steps: 26, steps per second: 120, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.346 [0.000, 1.000], mean observation: 0.031 [-1.541, 2.333], loss: 0.144559, mean_absolute_error: 1.283244, mean_q: 2.505975\n",
      " 221/1000: episode: 15, duration: 0.084s, episode steps: 10, steps per second: 119, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.135 [-1.607, 2.600], loss: 0.167926, mean_absolute_error: 1.369941, mean_q: 2.738532\n",
      " 232/1000: episode: 16, duration: 0.091s, episode steps: 11, steps per second: 121, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.151 [-1.132, 2.052], loss: 0.164101, mean_absolute_error: 1.425477, mean_q: 2.709371\n",
      " 255/1000: episode: 17, duration: 0.192s, episode steps: 23, steps per second: 120, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: -0.055 [-2.086, 1.351], loss: 0.175546, mean_absolute_error: 1.496199, mean_q: 2.974846\n",
      " 268/1000: episode: 18, duration: 0.108s, episode steps: 13, steps per second: 120, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.083 [-1.026, 1.606], loss: 0.199238, mean_absolute_error: 1.574834, mean_q: 3.045873\n",
      " 283/1000: episode: 19, duration: 0.125s, episode steps: 15, steps per second: 120, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.064 [-1.419, 2.184], loss: 0.172219, mean_absolute_error: 1.593844, mean_q: 3.189384\n",
      " 303/1000: episode: 20, duration: 0.166s, episode steps: 20, steps per second: 120, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.057 [-1.587, 2.434], loss: 0.187109, mean_absolute_error: 1.675244, mean_q: 3.284249\n",
      " 319/1000: episode: 21, duration: 0.132s, episode steps: 16, steps per second: 121, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.094 [-1.520, 2.485], loss: 0.162574, mean_absolute_error: 1.760664, mean_q: 3.526264\n",
      " 337/1000: episode: 22, duration: 0.150s, episode steps: 18, steps per second: 120, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.047 [-1.611, 2.415], loss: 0.208465, mean_absolute_error: 1.838159, mean_q: 3.600308\n",
      " 354/1000: episode: 23, duration: 0.142s, episode steps: 17, steps per second: 120, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.071 [-2.248, 1.394], loss: 0.227036, mean_absolute_error: 1.929496, mean_q: 3.777452\n",
      " 371/1000: episode: 24, duration: 0.140s, episode steps: 17, steps per second: 121, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.104 [-0.974, 1.888], loss: 0.200951, mean_absolute_error: 1.988885, mean_q: 3.945795\n",
      " 381/1000: episode: 25, duration: 0.083s, episode steps: 10, steps per second: 121, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.114 [-0.809, 1.389], loss: 0.198235, mean_absolute_error: 2.051106, mean_q: 4.017092\n",
      " 392/1000: episode: 26, duration: 0.092s, episode steps: 11, steps per second: 120, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.113 [-1.395, 2.265], loss: 0.185291, mean_absolute_error: 2.064497, mean_q: 4.025480\n",
      " 410/1000: episode: 27, duration: 0.150s, episode steps: 18, steps per second: 120, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.054 [-0.971, 1.532], loss: 0.198120, mean_absolute_error: 2.107785, mean_q: 4.122274\n",
      " 426/1000: episode: 28, duration: 0.133s, episode steps: 16, steps per second: 120, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.066 [-1.542, 2.453], loss: 0.208534, mean_absolute_error: 2.220246, mean_q: 4.333430\n",
      " 457/1000: episode: 29, duration: 0.259s, episode steps: 31, steps per second: 120, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.419 [0.000, 1.000], mean observation: 0.053 [-1.126, 1.828], loss: 0.166845, mean_absolute_error: 2.281821, mean_q: 4.498369\n",
      " 476/1000: episode: 30, duration: 0.157s, episode steps: 19, steps per second: 121, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.082 [-1.176, 2.040], loss: 0.165767, mean_absolute_error: 2.411575, mean_q: 4.744114\n",
      " 492/1000: episode: 31, duration: 0.133s, episode steps: 16, steps per second: 121, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.091 [-1.154, 1.842], loss: 0.211234, mean_absolute_error: 2.446057, mean_q: 4.799280\n",
      " 511/1000: episode: 32, duration: 0.159s, episode steps: 19, steps per second: 119, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.091 [-0.998, 1.827], loss: 0.173924, mean_absolute_error: 2.531103, mean_q: 4.968184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 526/1000: episode: 33, duration: 0.124s, episode steps: 15, steps per second: 120, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.100 [-1.327, 0.784], loss: 0.202913, mean_absolute_error: 2.581398, mean_q: 5.019850\n",
      " 538/1000: episode: 34, duration: 0.099s, episode steps: 12, steps per second: 121, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.100 [-1.388, 2.153], loss: 0.190828, mean_absolute_error: 2.677284, mean_q: 5.217380\n",
      " 549/1000: episode: 35, duration: 0.092s, episode steps: 11, steps per second: 120, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.138 [-1.803, 0.963], loss: 0.187702, mean_absolute_error: 2.733887, mean_q: 5.365613\n",
      " 570/1000: episode: 36, duration: 0.175s, episode steps: 21, steps per second: 120, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.090 [-0.762, 1.369], loss: 0.241432, mean_absolute_error: 2.764647, mean_q: 5.320617\n",
      " 597/1000: episode: 37, duration: 0.225s, episode steps: 27, steps per second: 120, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.407 [0.000, 1.000], mean observation: 0.055 [-0.995, 1.894], loss: 0.231215, mean_absolute_error: 2.854987, mean_q: 5.470462\n",
      " 630/1000: episode: 38, duration: 0.275s, episode steps: 33, steps per second: 120, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.036 [-1.372, 0.755], loss: 0.209041, mean_absolute_error: 2.956106, mean_q: 5.662616\n",
      " 653/1000: episode: 39, duration: 0.190s, episode steps: 23, steps per second: 121, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.115 [-0.606, 1.309], loss: 0.194684, mean_absolute_error: 3.072763, mean_q: 5.991630\n",
      " 671/1000: episode: 40, duration: 0.150s, episode steps: 18, steps per second: 120, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.093 [-0.559, 1.031], loss: 0.198036, mean_absolute_error: 3.145254, mean_q: 6.102283\n",
      " 697/1000: episode: 41, duration: 0.217s, episode steps: 26, steps per second: 120, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.577 [0.000, 1.000], mean observation: -0.095 [-1.839, 0.837], loss: 0.252746, mean_absolute_error: 3.206566, mean_q: 6.128812\n",
      " 716/1000: episode: 42, duration: 0.158s, episode steps: 19, steps per second: 120, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.085 [-1.013, 1.750], loss: 0.183921, mean_absolute_error: 3.305128, mean_q: 6.425036\n",
      " 727/1000: episode: 43, duration: 0.091s, episode steps: 11, steps per second: 121, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.109 [-1.200, 1.994], loss: 0.226194, mean_absolute_error: 3.308352, mean_q: 6.353111\n",
      " 745/1000: episode: 44, duration: 0.150s, episode steps: 18, steps per second: 120, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.093 [-0.813, 1.616], loss: 0.244477, mean_absolute_error: 3.392076, mean_q: 6.551602\n",
      " 767/1000: episode: 45, duration: 0.183s, episode steps: 22, steps per second: 121, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.096 [-1.076, 0.553], loss: 0.261531, mean_absolute_error: 3.426112, mean_q: 6.589064\n",
      " 781/1000: episode: 46, duration: 0.117s, episode steps: 14, steps per second: 119, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.073 [-1.171, 1.773], loss: 0.330151, mean_absolute_error: 3.475819, mean_q: 6.677883\n",
      " 794/1000: episode: 47, duration: 0.108s, episode steps: 13, steps per second: 121, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.104 [-1.010, 1.839], loss: 0.322560, mean_absolute_error: 3.535544, mean_q: 6.788104\n",
      " 869/1000: episode: 48, duration: 0.626s, episode steps: 75, steps per second: 120, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.039 [-1.126, 1.451], loss: 0.222296, mean_absolute_error: 3.665023, mean_q: 7.090178\n",
      " 916/1000: episode: 49, duration: 0.391s, episode steps: 47, steps per second: 120, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: -0.013 [-1.172, 0.939], loss: 0.200983, mean_absolute_error: 3.835486, mean_q: 7.474866\n",
      " 952/1000: episode: 50, duration: 0.301s, episode steps: 36, steps per second: 120, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.072 [-0.813, 1.691], loss: 0.188088, mean_absolute_error: 4.008051, mean_q: 7.837528\n",
      " 965/1000: episode: 51, duration: 0.108s, episode steps: 13, steps per second: 121, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.083 [-1.007, 1.594], loss: 0.179776, mean_absolute_error: 4.133817, mean_q: 8.145617\n",
      " 983/1000: episode: 52, duration: 0.149s, episode steps: 18, steps per second: 121, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.094 [-1.271, 0.803], loss: 0.190299, mean_absolute_error: 4.063879, mean_q: 8.017589\n",
      "done, took 9.716 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ebeb93ba08>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=1000, visualize=True, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wagi które uzyskaliśmy w wyniku trenowania zapisuje w osobnym pliku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('dqn_{}_weights.h5f'.format('CartPole-v0'), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dokonujemy testu sieci przy 10 iteracjach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 173.000, steps: 173\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 184.000, steps: 184\n",
      "Episode 5: reward: 161.000, steps: 161\n",
      "Episode 6: reward: 200.000, steps: 200\n",
      "Episode 7: reward: 157.000, steps: 157\n",
      "Episode 8: reward: 182.000, steps: 182\n",
      "Episode 9: reward: 162.000, steps: 162\n",
      "Episode 10: reward: 200.000, steps: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ebffb03208>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorytm DQN dopasował idealnie wszystkie wagi do funkcji i uzyskujemy maksymalny wynik możliwy dla ilości momentów w czasie.\n",
    "Możemy również zauważyć, że korzystanie z możliwosći karty graficznej do trenowania tej sieci jest nieporównalnie szybsze od poprzedniej metody w notebooku nr 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
